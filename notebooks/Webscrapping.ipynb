{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504c58f1-9fb0-45d1-a73d-bb896ba73edb",
   "metadata": {},
   "source": [
    "### In this exercise, you'll practice using BeautifulSoup to parse the content of a web page. The page that you'll be scraping, https://realpython.github.io/fake-jobs/, contains job listings. Your job is to extract the data on each job and convert into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b247a7-8d7f-41f0-a1a3-a5ae4bb10bac",
   "metadata": {},
   "source": [
    "In this notebook, we'll see how we can retrieve the contents of a website and then parse the resulting HTML to extract the data we want.\n",
    "\n",
    "For this, we'll again be using the requests library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eed9bf9-fc37-45e0-b8c5-a71130d95db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14516daf-66fe-4099-a6fa-7c2fd346db3e",
   "metadata": {},
   "source": [
    "# Question 1-.\n",
    "Start by performing a GET request on the url above and convert the response into a BeautifulSoup object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0350fa8e-76a4-43ce-8dd6-adfd187d33bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://realpython.github.io/fake-jobs/'\n",
    "\n",
    "response = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf6298c5-8035-4008-a9bb-525ab2914f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f2fe42-e681-45e5-a70a-f0aa9fc93172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629aa336-7f25-4a3d-96c5-9004944db800",
   "metadata": {},
   "source": [
    "# Question 1a.\n",
    "Use the .find method to find the tag containing the first job title (\"Senior Python Developer\"). Hint: can you find a tag type and/or a class that could be helpful for extracting this information? Extract the text from this title. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f582ddf5-9a0d-4816-9309-23d06751cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d135b860-6c05-48d6-a977-2759cbd5bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e53eb1e5-aeaf-48dd-a580-b069a49c2e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0bbf14f-d98b-4d89-a05e-20eb28535799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h2 class=\"title is-5\">Senior Python Developer</h2>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e95453c-759a-44f7-ac8b-327ca50e4ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup.find('h2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a541918-71b8-4fb2-8267-9275d59693ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Senior Python Developer'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('h2').text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4998c8-0a96-459c-9a06-d07a5f323127",
   "metadata": {},
   "source": [
    "# Question 1b.\n",
    "Now, use what you did for the first title, but extract the job title for all jobs on this page. Store the results in a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31565822-44b4-42a4-9265-731e490accaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Senior Python Developer',\n",
       " 'Energy engineer',\n",
       " 'Legal executive',\n",
       " 'Fitness centre manager',\n",
       " 'Product manager',\n",
       " 'Medical technical officer',\n",
       " 'Physiological scientist',\n",
       " 'Textile designer',\n",
       " 'Television floor manager',\n",
       " 'Waste management officer',\n",
       " 'Software Engineer (Python)',\n",
       " 'Interpreter',\n",
       " 'Architect',\n",
       " 'Meteorologist',\n",
       " 'Audiological scientist',\n",
       " 'English as a second language teacher',\n",
       " 'Surgeon',\n",
       " 'Equities trader',\n",
       " 'Newspaper journalist',\n",
       " 'Materials engineer',\n",
       " 'Python Programmer (Entry-Level)',\n",
       " 'Product/process development scientist',\n",
       " 'Scientist, research (maths)',\n",
       " 'Ecologist',\n",
       " 'Materials engineer',\n",
       " 'Historic buildings inspector/conservation officer',\n",
       " 'Data scientist',\n",
       " 'Psychiatrist',\n",
       " 'Structural engineer',\n",
       " 'Immigration officer',\n",
       " 'Python Programmer (Entry-Level)',\n",
       " 'Neurosurgeon',\n",
       " 'Broadcast engineer',\n",
       " 'Make',\n",
       " 'Nurse, adult',\n",
       " 'Air broker',\n",
       " 'Editor, film/video',\n",
       " 'Production assistant, radio',\n",
       " 'Engineer, communications',\n",
       " 'Sales executive',\n",
       " 'Software Developer (Python)',\n",
       " 'Futures trader',\n",
       " 'Tour manager',\n",
       " 'Cytogeneticist',\n",
       " 'Designer, multimedia',\n",
       " 'Trade union research officer',\n",
       " 'Chemist, analytical',\n",
       " 'Programmer, multimedia',\n",
       " 'Engineer, broadcasting (operations)',\n",
       " 'Teacher, primary school',\n",
       " 'Python Developer',\n",
       " 'Manufacturing systems engineer',\n",
       " 'Producer, television/film/video',\n",
       " 'Scientist, forensic',\n",
       " 'Bonds trader',\n",
       " 'Editorial assistant',\n",
       " 'Photographer',\n",
       " 'Retail banker',\n",
       " 'Jewellery designer',\n",
       " 'Ophthalmologist',\n",
       " 'Back-End Web Developer (Python, Django)',\n",
       " 'Licensed conveyancer',\n",
       " 'Futures trader',\n",
       " 'Counselling psychologist',\n",
       " 'Insurance underwriter',\n",
       " 'Engineer, automotive',\n",
       " 'Producer, radio',\n",
       " 'Dispensing optician',\n",
       " 'Designer, fashion/clothing',\n",
       " 'Chartered loss adjuster',\n",
       " 'Back-End Web Developer (Python, Django)',\n",
       " 'Forest/woodland manager',\n",
       " 'Clinical cytogeneticist',\n",
       " 'Print production planner',\n",
       " 'Systems developer',\n",
       " 'Graphic designer',\n",
       " 'Writer',\n",
       " 'Field seismologist',\n",
       " 'Chief Strategy Officer',\n",
       " 'Air cabin crew',\n",
       " 'Python Programmer (Entry-Level)',\n",
       " 'Warden/ranger',\n",
       " 'Sports therapist',\n",
       " 'Arts development officer',\n",
       " 'Printmaker',\n",
       " 'Health and safety adviser',\n",
       " 'Manufacturing systems engineer',\n",
       " 'Programmer, applications',\n",
       " 'Medical physicist',\n",
       " 'Media planner',\n",
       " 'Software Developer (Python)',\n",
       " 'Surveyor, land/geomatics',\n",
       " 'Legal executive',\n",
       " 'Librarian, academic',\n",
       " 'Barrister',\n",
       " 'Museum/gallery exhibitions officer',\n",
       " 'Radiographer, diagnostic',\n",
       " 'Database administrator',\n",
       " 'Furniture designer',\n",
       " 'Ship broker']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_titles = soup.findAll('h2')\n",
    "job_title = [job_title.text.strip() for job_title in job_titles]\n",
    "job_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf1c020-760f-4acf-b767-fad5c9dc345a",
   "metadata": {},
   "source": [
    "# Question 1c.\n",
    "Finally, extract the companies, locations, and posting dates for each job. For example, the first job has a company of \"Payne, Roberts and Davis\", a location of \"Stewartbury, AA\", and a posting date of \"2021-04-08\". Ensure that the text that you extract is clean, meaning no extra spaces or other characters at the beginning or end.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22d1c2ea-77b3-48fe-890c-f636fc5a4cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h3 class=\"subtitle is-6 company\">Payne, Roberts and Davis</h3>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db8e2257-3309-4c91-866b-b6071d8927ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Payne, Roberts and Davis'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('h3').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89413d30-52c2-4809-a1e8-94e709961772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Payne, Roberts and Davis',\n",
       " 'Vasquez-Davidson',\n",
       " 'Jackson, Chambers and Levy',\n",
       " 'Savage-Bradley',\n",
       " 'Ramirez Inc',\n",
       " 'Rogers-Yates',\n",
       " 'Kramer-Klein',\n",
       " 'Meyers-Johnson',\n",
       " 'Hughes-Williams',\n",
       " 'Jones, Williams and Villa',\n",
       " 'Garcia PLC',\n",
       " 'Gregory and Sons',\n",
       " 'Clark, Garcia and Sosa',\n",
       " 'Bush PLC',\n",
       " 'Salazar-Meyers',\n",
       " 'Parker, Murphy and Brooks',\n",
       " 'Cruz-Brown',\n",
       " 'Macdonald-Ferguson',\n",
       " 'Williams, Peterson and Rojas',\n",
       " 'Smith and Sons',\n",
       " 'Moss, Duncan and Allen',\n",
       " 'Gomez-Carroll',\n",
       " 'Manning, Welch and Herring',\n",
       " 'Lee, Gutierrez and Brown',\n",
       " 'Davis, Serrano and Cook',\n",
       " 'Smith LLC',\n",
       " 'Thomas Group',\n",
       " 'Silva-King',\n",
       " 'Pierce-Long',\n",
       " 'Walker-Simpson',\n",
       " 'Cooper and Sons',\n",
       " 'Donovan, Gonzalez and Figueroa',\n",
       " 'Morgan, Butler and Bennett',\n",
       " 'Snyder-Lee',\n",
       " 'Harris PLC',\n",
       " 'Washington PLC',\n",
       " 'Brown, Price and Campbell',\n",
       " 'Mcgee PLC',\n",
       " 'Dixon Inc',\n",
       " 'Thompson, Sheppard and Ward',\n",
       " 'Adams-Brewer',\n",
       " 'Schneider-Brady',\n",
       " 'Gonzales-Frank',\n",
       " 'Smith-Wong',\n",
       " 'Pierce-Herrera',\n",
       " 'Aguilar, Rivera and Quinn',\n",
       " 'Lowe, Barnes and Thomas',\n",
       " 'Lewis, Gonzalez and Vasquez',\n",
       " 'Taylor PLC',\n",
       " 'Oliver, Jones and Ramirez',\n",
       " 'Rivera and Sons',\n",
       " 'Garcia PLC',\n",
       " 'Johnson, Wells and Kramer',\n",
       " 'Gonzalez LLC',\n",
       " 'Morgan, White and Macdonald',\n",
       " 'Robinson-Fitzpatrick',\n",
       " 'Waters, Wilson and Hoover',\n",
       " 'Hill LLC',\n",
       " 'Li-Gregory',\n",
       " 'Fisher, Ryan and Coleman',\n",
       " 'Stewart-Alexander',\n",
       " 'Abbott and Sons',\n",
       " 'Bryant, Santana and Davenport',\n",
       " 'Smith PLC',\n",
       " 'Patterson-Singh',\n",
       " 'Martinez-Berry',\n",
       " 'May, Taylor and Fisher',\n",
       " 'Bailey, Owen and Thompson',\n",
       " 'Vasquez Ltd',\n",
       " 'Leblanc LLC',\n",
       " 'Jackson, Ali and Mckee',\n",
       " 'Blankenship, Knight and Powell',\n",
       " 'Patton, Haynes and Jones',\n",
       " 'Wood Inc',\n",
       " 'Collins Group',\n",
       " 'Flores-Nelson',\n",
       " 'Mitchell, Jones and Olson',\n",
       " 'Howard Group',\n",
       " 'Kramer-Edwards',\n",
       " 'Berry-Houston',\n",
       " 'Mathews Inc',\n",
       " 'Riley-Johnson',\n",
       " 'Spencer and Sons',\n",
       " 'Camacho-Sanchez',\n",
       " 'Oliver and Sons',\n",
       " 'Eaton PLC',\n",
       " 'Stanley-Frederick',\n",
       " 'Bradley LLC',\n",
       " 'Parker, Goodwin and Zavala',\n",
       " 'Kim-Miles',\n",
       " 'Moreno-Rodriguez',\n",
       " 'Brown-Ortiz',\n",
       " 'Hartman PLC',\n",
       " 'Brooks Inc',\n",
       " 'Washington-Castillo',\n",
       " 'Nguyen, Yoder and Petty',\n",
       " 'Holder LLC',\n",
       " 'Yates-Ferguson',\n",
       " 'Ortega-Lawrence',\n",
       " 'Fuentes, Walls and Castro']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies = soup.findAll('h3')\n",
    "company = [company.text.strip() for company in companies]\n",
    "company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01cefc3c-07de-4dbe-8d06-89eb81b5e248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p class=\"subtitle is-3\">\n",
       "        Fake Jobs for Your Web Scraping Journey\n",
       "      </p>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5c8de3e-1dbd-4b05-be40-290f89c99363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fake Jobs for Your Web Scraping Journey'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p').text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83f92a01-71c4-4125-9919-e3d5095353d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"location\">\n",
       "         Stewartbury, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Christopherville, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Port Ericaburgh, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         East Seanview, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         North Jamieview, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Davidville, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         South Christopher, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Port Jonathan, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Osbornetown, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Scotttown, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Ericberg, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Ramireztown, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Figueroaview, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Kelseystad, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Williamsburgh, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Mitchellburgh, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         West Jessicabury, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Maloneshire, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Johnsonton, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         South Davidtown, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Port Sara, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Marktown, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Laurenland, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Lauraton, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         South Tammyberg, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         North Brandonville, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Port Robertfurt, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Burnettbury, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Herbertside, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Christopherport, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         West Victor, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Port Aaron, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Loribury, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Angelastad, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Larrytown, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         West Colin, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         West Stephanie, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Laurentown, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Wrightberg, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Alberttown, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Brockburgh, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         North Jason, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Arnoldhaven, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Lake Destiny, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         South Timothyburgh, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         New Jimmyton, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         New Lucasbury, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Port Cory, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Gileston, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Cindyshire, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         East Michaelfort, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Joybury, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Emmatown, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Colehaven, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Port Coryton, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Amyborough, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Reynoldsville, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Port Billy, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Adamburgh, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Wilsonmouth, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         South Kimberly, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Benjaminland, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Zacharyport, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Port Devonville, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         East Thomas, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         New Jeffrey, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Davidside, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Jamesville, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         New Kelly, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Lake Antonio, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         New Elizabethside, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Millsbury, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Lloydton, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Port Jeremy, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         New Elizabethtown, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Charlesstad, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Josephbury, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Seanfurt, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Williambury, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         South Jorgeside, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Robertborough, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         South Saratown, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Hullview, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Philipland, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         North Patty, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         North Stephen, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Stevensland, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Reyesstad, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Bellberg, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         North Johnland, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Martinezburgh, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Joshuatown, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         West Ericstad, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Tuckertown, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Perezton, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Lake Abigail, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Jacobshire, AP\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Port Susan, AE\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         North Tiffany, AA\n",
       "       </p>,\n",
       " <p class=\"location\">\n",
       "         Michelleville, AP\n",
       "       </p>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.findAll('p', class_='location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db702b91-99aa-4f49-a0ba-b9be6ebdc156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stewartbury, AA',\n",
       " 'Christopherville, AA',\n",
       " 'Port Ericaburgh, AA',\n",
       " 'East Seanview, AP',\n",
       " 'North Jamieview, AP',\n",
       " 'Davidville, AP',\n",
       " 'South Christopher, AE',\n",
       " 'Port Jonathan, AE',\n",
       " 'Osbornetown, AE',\n",
       " 'Scotttown, AP',\n",
       " 'Ericberg, AE',\n",
       " 'Ramireztown, AE',\n",
       " 'Figueroaview, AA',\n",
       " 'Kelseystad, AA',\n",
       " 'Williamsburgh, AE',\n",
       " 'Mitchellburgh, AE',\n",
       " 'West Jessicabury, AA',\n",
       " 'Maloneshire, AE',\n",
       " 'Johnsonton, AA',\n",
       " 'South Davidtown, AP',\n",
       " 'Port Sara, AE',\n",
       " 'Marktown, AA',\n",
       " 'Laurenland, AE',\n",
       " 'Lauraton, AP',\n",
       " 'South Tammyberg, AP',\n",
       " 'North Brandonville, AP',\n",
       " 'Port Robertfurt, AA',\n",
       " 'Burnettbury, AE',\n",
       " 'Herbertside, AA',\n",
       " 'Christopherport, AP',\n",
       " 'West Victor, AE',\n",
       " 'Port Aaron, AP',\n",
       " 'Loribury, AA',\n",
       " 'Angelastad, AP',\n",
       " 'Larrytown, AE',\n",
       " 'West Colin, AP',\n",
       " 'West Stephanie, AP',\n",
       " 'Laurentown, AP',\n",
       " 'Wrightberg, AP',\n",
       " 'Alberttown, AE',\n",
       " 'Brockburgh, AE',\n",
       " 'North Jason, AE',\n",
       " 'Arnoldhaven, AE',\n",
       " 'Lake Destiny, AP',\n",
       " 'South Timothyburgh, AP',\n",
       " 'New Jimmyton, AE',\n",
       " 'New Lucasbury, AP',\n",
       " 'Port Cory, AE',\n",
       " 'Gileston, AA',\n",
       " 'Cindyshire, AA',\n",
       " 'East Michaelfort, AA',\n",
       " 'Joybury, AE',\n",
       " 'Emmatown, AE',\n",
       " 'Colehaven, AP',\n",
       " 'Port Coryton, AE',\n",
       " 'Amyborough, AA',\n",
       " 'Reynoldsville, AA',\n",
       " 'Port Billy, AP',\n",
       " 'Adamburgh, AA',\n",
       " 'Wilsonmouth, AA',\n",
       " 'South Kimberly, AA',\n",
       " 'Benjaminland, AP',\n",
       " 'Zacharyport, AA',\n",
       " 'Port Devonville, AE',\n",
       " 'East Thomas, AE',\n",
       " 'New Jeffrey, AP',\n",
       " 'Davidside, AA',\n",
       " 'Jamesville, AA',\n",
       " 'New Kelly, AP',\n",
       " 'Lake Antonio, AA',\n",
       " 'New Elizabethside, AA',\n",
       " 'Millsbury, AE',\n",
       " 'Lloydton, AP',\n",
       " 'Port Jeremy, AA',\n",
       " 'New Elizabethtown, AA',\n",
       " 'Charlesstad, AE',\n",
       " 'Josephbury, AE',\n",
       " 'Seanfurt, AA',\n",
       " 'Williambury, AA',\n",
       " 'South Jorgeside, AP',\n",
       " 'Robertborough, AP',\n",
       " 'South Saratown, AP',\n",
       " 'Hullview, AA',\n",
       " 'Philipland, AP',\n",
       " 'North Patty, AE',\n",
       " 'North Stephen, AE',\n",
       " 'Stevensland, AP',\n",
       " 'Reyesstad, AE',\n",
       " 'Bellberg, AP',\n",
       " 'North Johnland, AE',\n",
       " 'Martinezburgh, AE',\n",
       " 'Joshuatown, AE',\n",
       " 'West Ericstad, AA',\n",
       " 'Tuckertown, AE',\n",
       " 'Perezton, AE',\n",
       " 'Lake Abigail, AE',\n",
       " 'Jacobshire, AP',\n",
       " 'Port Susan, AE',\n",
       " 'North Tiffany, AA',\n",
       " 'Michelleville, AP']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locs = soup.findAll('p', class_='location')\n",
    "location = [location.text.strip() for location in locs]\n",
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e019382-9e06-4c80-bf95-8c3064e18e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<time datetime=\"2021-04-08\">2021-04-08</time>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffa30d33-deba-4c4e-8bad-5ec325fec7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-04-08'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('time').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7353189-a9e4-4714-ab56-4b5e013d9aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = soup.findAll('time')\n",
    "date = [date.text.strip() for date in dat]\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e90b1-42a9-48da-b541-85e172087e3f",
   "metadata": {},
   "source": [
    "# Question 1d. \n",
    "Take the lists that you have created and combine them into a pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4b2859d-729c-4720-a8fd-5c8fe02eaac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6cc4b7e-d183-4f75-abf4-b4cd3891574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#table_html = str(soup.find('table', attrs={'class' : 'wikitable'}))\n",
    "\n",
    "#from IPython.core.display import HTML\n",
    "\n",
    "#HTML(table_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4efd01fb-7c07-40e6-acb4-902113665dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'col1': job_title, 'col2': company, 'col3' : date, 'col4': location})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "991c1f76-a074-4362-8d3d-3d801bd7f51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8251cc49-9ac3-43c3-b805-15436e954720",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'col1': 'job_titles', 'col2': 'company', 'col3':'date', 'col4':'location'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55127704-6d60-4f25-bfe8-8ef2192af9b1",
   "metadata": {},
   "source": [
    "# Question 2. \n",
    "# Next, add a column that contains the url for the \"Apply\" button. Try this in two ways.   \n",
    "    a. First, use the BeautifulSoup find_all method to extract the urls.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83533bb8-25e0-420f-bf40-c2799d143053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"card-footer-item\" href=\"https://www.realpython.com\" target=\"_blank\">Learn</a>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "975fe83b-0415-4d2c-ac08-730c5f150e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all links for anchor tag and specific \"Apply\" button\n",
    "#links = soup.find_all('a', string='Apply')\n",
    "# create a blank list links_list\n",
    "#links_list = []\n",
    "#for link in links:\n",
    "    # Create a list of all the URL's for Apply button\n",
    "#    links_list.append(link.get('href'))\n",
    "# print final list of links\n",
    "#links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab5fed8c-295d-40bb-942d-b69fd23bd00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"card-footer-item\" href=\"https://www.realpython.com\" target=\"_blank\">Learn</a>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a', string = 'Learn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48399e1f-ccbf-455b-857a-fbc95cc73e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"card-footer-item\" href=\"https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html\" target=\"_blank\">Apply</a>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a', string = 'Apply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e81c20ef-ccfa-460d-ad01-ad44281aa25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[a['href'] for a in soup.findAll('a', string = 'Learn')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a674b82a-ac2e-4f88-af8b-f2156c301a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/energy-engineer-1.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/legal-executive-2.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/fitness-centre-manager-3.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/product-manager-4.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/medical-technical-officer-5.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/physiological-scientist-6.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/textile-designer-7.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/television-floor-manager-8.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/waste-management-officer-9.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/software-engineer-python-10.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/interpreter-11.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/architect-12.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/meteorologist-13.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/audiological-scientist-14.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/english-as-a-second-language-teacher-15.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/surgeon-16.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/equities-trader-17.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/newspaper-journalist-18.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/materials-engineer-19.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/python-programmer-entry-level-20.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/product-process-development-scientist-21.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/scientist-research-maths-22.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/ecologist-23.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/materials-engineer-24.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/historic-buildings-inspector-conservation-officer-25.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/data-scientist-26.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/psychiatrist-27.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/structural-engineer-28.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/immigration-officer-29.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/python-programmer-entry-level-30.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/neurosurgeon-31.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/broadcast-engineer-32.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/make-33.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/nurse-adult-34.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/air-broker-35.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/editor-film-video-36.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/production-assistant-radio-37.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/engineer-communications-38.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/sales-executive-39.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/software-developer-python-40.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/futures-trader-41.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/tour-manager-42.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/cytogeneticist-43.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/designer-multimedia-44.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/trade-union-research-officer-45.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/chemist-analytical-46.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/programmer-multimedia-47.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/engineer-broadcasting-operations-48.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/teacher-primary-school-49.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/python-developer-50.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/manufacturing-systems-engineer-51.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/producer-television-film-video-52.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/scientist-forensic-53.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/bonds-trader-54.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/editorial-assistant-55.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/photographer-56.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/retail-banker-57.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/jewellery-designer-58.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/ophthalmologist-59.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/back-end-web-developer-python-django-60.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/licensed-conveyancer-61.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/futures-trader-62.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/counselling-psychologist-63.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/insurance-underwriter-64.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/engineer-automotive-65.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/producer-radio-66.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/dispensing-optician-67.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/designer-fashion-clothing-68.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/chartered-loss-adjuster-69.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/back-end-web-developer-python-django-70.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/forest-woodland-manager-71.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/clinical-cytogeneticist-72.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/print-production-planner-73.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/systems-developer-74.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/graphic-designer-75.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/writer-76.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/field-seismologist-77.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/chief-strategy-officer-78.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/air-cabin-crew-79.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/python-programmer-entry-level-80.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/warden-ranger-81.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/sports-therapist-82.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/arts-development-officer-83.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/printmaker-84.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/health-and-safety-adviser-85.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/manufacturing-systems-engineer-86.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/programmer-applications-87.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/medical-physicist-88.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/media-planner-89.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/software-developer-python-90.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/surveyor-land-geomatics-91.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/legal-executive-92.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/librarian-academic-93.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/barrister-94.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/museum-gallery-exhibitions-officer-95.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/radiographer-diagnostic-96.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/database-administrator-97.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/furniture-designer-98.html',\n",
       " 'https://realpython.github.io/fake-jobs/jobs/ship-broker-99.html']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply = [a['href'] for a in soup.findAll('a', string = 'Apply')]\n",
    "apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a826ce38-feb8-4ff2-a4e6-f2c3ab9fbef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_titles</th>\n",
       "      <th>company</th>\n",
       "      <th>date</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Python Developer</td>\n",
       "      <td>Payne, Roberts and Davis</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>Stewartbury, AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Energy engineer</td>\n",
       "      <td>Vasquez-Davidson</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>Christopherville, AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Legal executive</td>\n",
       "      <td>Jackson, Chambers and Levy</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>Port Ericaburgh, AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fitness centre manager</td>\n",
       "      <td>Savage-Bradley</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>East Seanview, AP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product manager</td>\n",
       "      <td>Ramirez Inc</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>North Jamieview, AP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Museum/gallery exhibitions officer</td>\n",
       "      <td>Nguyen, Yoder and Petty</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>Lake Abigail, AE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Radiographer, diagnostic</td>\n",
       "      <td>Holder LLC</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>Jacobshire, AP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Database administrator</td>\n",
       "      <td>Yates-Ferguson</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>Port Susan, AE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Furniture designer</td>\n",
       "      <td>Ortega-Lawrence</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>North Tiffany, AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Ship broker</td>\n",
       "      <td>Fuentes, Walls and Castro</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>Michelleville, AP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            job_titles                     company  \\\n",
       "0              Senior Python Developer    Payne, Roberts and Davis   \n",
       "1                      Energy engineer            Vasquez-Davidson   \n",
       "2                      Legal executive  Jackson, Chambers and Levy   \n",
       "3               Fitness centre manager              Savage-Bradley   \n",
       "4                      Product manager                 Ramirez Inc   \n",
       "..                                 ...                         ...   \n",
       "95  Museum/gallery exhibitions officer     Nguyen, Yoder and Petty   \n",
       "96            Radiographer, diagnostic                  Holder LLC   \n",
       "97              Database administrator              Yates-Ferguson   \n",
       "98                  Furniture designer             Ortega-Lawrence   \n",
       "99                         Ship broker   Fuentes, Walls and Castro   \n",
       "\n",
       "          date              location  \n",
       "0   2021-04-08       Stewartbury, AA  \n",
       "1   2021-04-08  Christopherville, AA  \n",
       "2   2021-04-08   Port Ericaburgh, AA  \n",
       "3   2021-04-08     East Seanview, AP  \n",
       "4   2021-04-08   North Jamieview, AP  \n",
       "..         ...                   ...  \n",
       "95  2021-04-08      Lake Abigail, AE  \n",
       "96  2021-04-08        Jacobshire, AP  \n",
       "97  2021-04-08        Port Susan, AE  \n",
       "98  2021-04-08     North Tiffany, AA  \n",
       "99  2021-04-08     Michelleville, AP  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d459ba26-6e34-4797-b48d-b24076c1c8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['url']= apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84843d5-3edf-43ab-aab5-b43d14ce1149",
   "metadata": {},
   "source": [
    "# Question 2b\n",
    "b. Next, get those same urls in a different way. Examine the urls and see if you can spot the pattern of how they are constructed. Then, build the url using the elements you have already extracted. Ensure that the urls that you created match those that you extracted using BeautifulSoup. Warning: You will need to do some string cleaning and prep in constructing the urls this way. For example, look carefully at the urls for the \"Software Engineer (Python)\" job and the \"Scientist, research (maths)\" job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8ce0601-b912-49a9-a4fb-4b580c960b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.2.b. #url is constructed base url = 'https://realpython.github.io/fake-jobs'  and   /jobs/  and job title  and .html \n",
    "# if the title has () it is replaced with -  .\n",
    "#job_titles # job titles from q.1\n",
    "#base_url = 'https://realpython.github.io/fake-jobs'\n",
    "#constructed_url = [base_url + '/jobs/' +title.replace(' ','-').replace(',','').replace('(','').replace(')','').lower() + f'-{index}.html' for index,title in enumerate(job_titles)]\n",
    "#enumerate used to iterate over a sequence ,while keeping track of index of each item it returns both index and item\n",
    "#constructed_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3035cc82-8204-4ef4-917d-4aa9e472d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2b. translate method\n",
    "import string\n",
    "base_url = 'https://realpython.github.io/fake-jobs/jobs/'\n",
    "url_title = [x.replace('/',' ').translate(str.maketrans('','', string.punctuation)).lower().replace(' ','-') for x in job_title]\n",
    "url_2 = [f'{base_url}{x}-{i}.html' for i, x in enumerate(url_title)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1eab9048-d1f9-48c7-ac6d-a346a442bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd method\n",
    "#jobs['url2'] = 'https://realpython.github.io/fake-jobs/jobs/' + (\n",
    "#    jobs['title']\n",
    "#    .str.lower()\n",
    "#    .str.replace('[\\s/]', '-', regex = True)\n",
    "#    .str.replace('[(),]', '', regex = True)\n",
    "#) + '-' + jobs.index.astype(str) + '.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e471bb41-e563-44f0-a29f-4809b3d62e6c",
   "metadata": {},
   "source": [
    "# question 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a55049c-e8f4-4ee1-a9a4-4016f80fffa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Professional asset web application environmentally friendly detail-oriented asset. Coordinate educational dashboard agile employ growth opportunity. Company programs CSS explore role. Html educational grit web application. Oversea SCRUM talented support. Web Application fast-growing communities inclusive programs job CSS. Css discussions growth opportunity explore open-minded oversee. Css Python environmentally friendly collaborate inclusive role. Django no experience oversee dashboard environmentally friendly willing to learn programs. Programs open-minded programs asset.\n"
     ]
    }
   ],
   "source": [
    "# import urllib module\n",
    "import urllib.request \n",
    "\n",
    "# providing url \n",
    "url = \"https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html\" \n",
    "\n",
    "# opening the url for reading \n",
    "html = urllib.request.urlopen(url) \n",
    "  \n",
    "# parsing the html file \n",
    "htmlParse = BeautifulSoup(html, 'html.parser') \n",
    "  \n",
    "# getting all the paragraphs \n",
    "for para in htmlParse.find_all(\"p\")[1]:\n",
    "    print(para.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4abd45c-f58e-4b5d-a393-399afdedd290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Professional asset web application environmentally friendly detail-oriented asset. Coordinate educational dashboard agile employ growth opportunity. Company programs CSS explore role. Html educational grit web application. Oversea SCRUM talented support. Web Application fast-growing communities inclusive programs job CSS. Css discussions growth opportunity explore open-minded oversee. Css Python environmentally friendly collaborate inclusive role. Django no experience oversee dashboard environmentally friendly willing to learn programs. Programs open-minded programs asset.\n"
     ]
    }
   ],
   "source": [
    "#Method 2 \n",
    "for div in htmlParse.find_all(\"div\", class_=\"content\"):\n",
    "    para = div.find_all(\"p\")[0]\n",
    "    print(para.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a983ab7-9f44-4d74-9bd7-5b5e4039a9cb",
   "metadata": {},
   "source": [
    "# Question 3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bcde8db0-b8c7-47a0-a335-36b5935a46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptions(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    description = soup.findAll('p')[1]\n",
    "    return description.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc89bbf7-6b2e-42e8-8a8c-dcb60ff23092",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description']=df['url'].apply(get_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01f967dc-9d1d-4556-8199-342525135b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_titles</th>\n",
       "      <th>company</th>\n",
       "      <th>date</th>\n",
       "      <th>location</th>\n",
       "      <th>url</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Python Developer</td>\n",
       "      <td>Payne, Roberts and Davis</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>Stewartbury, AA</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/se...</td>\n",
       "      <td>Professional asset web application environment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Energy engineer</td>\n",
       "      <td>Vasquez-Davidson</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>Christopherville, AA</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/en...</td>\n",
       "      <td>Party prevent live. Quickly candidate change a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Legal executive</td>\n",
       "      <td>Jackson, Chambers and Levy</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>Port Ericaburgh, AA</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/le...</td>\n",
       "      <td>Administration even relate head color. Staff b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fitness centre manager</td>\n",
       "      <td>Savage-Bradley</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>East Seanview, AP</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/fi...</td>\n",
       "      <td>Tv program actually race tonight themselves tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product manager</td>\n",
       "      <td>Ramirez Inc</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>North Jamieview, AP</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/pr...</td>\n",
       "      <td>Traditional page a although for study anyone. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Museum/gallery exhibitions officer</td>\n",
       "      <td>Nguyen, Yoder and Petty</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>Lake Abigail, AE</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/mu...</td>\n",
       "      <td>Paper age physical current note. There reality...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Radiographer, diagnostic</td>\n",
       "      <td>Holder LLC</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>Jacobshire, AP</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/ra...</td>\n",
       "      <td>Able such right culture. Wrong pick structure ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Database administrator</td>\n",
       "      <td>Yates-Ferguson</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>Port Susan, AE</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/da...</td>\n",
       "      <td>Create day party decade high clear. Past trade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Furniture designer</td>\n",
       "      <td>Ortega-Lawrence</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>North Tiffany, AA</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/fu...</td>\n",
       "      <td>Pressure under rock next week. Recognize so re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Ship broker</td>\n",
       "      <td>Fuentes, Walls and Castro</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>Michelleville, AP</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/sh...</td>\n",
       "      <td>Management common popular project only. Must s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            job_titles                     company  \\\n",
       "0              Senior Python Developer    Payne, Roberts and Davis   \n",
       "1                      Energy engineer            Vasquez-Davidson   \n",
       "2                      Legal executive  Jackson, Chambers and Levy   \n",
       "3               Fitness centre manager              Savage-Bradley   \n",
       "4                      Product manager                 Ramirez Inc   \n",
       "..                                 ...                         ...   \n",
       "95  Museum/gallery exhibitions officer     Nguyen, Yoder and Petty   \n",
       "96            Radiographer, diagnostic                  Holder LLC   \n",
       "97              Database administrator              Yates-Ferguson   \n",
       "98                  Furniture designer             Ortega-Lawrence   \n",
       "99                         Ship broker   Fuentes, Walls and Castro   \n",
       "\n",
       "          date              location  \\\n",
       "0   2021-04-08       Stewartbury, AA   \n",
       "1   2021-04-08  Christopherville, AA   \n",
       "2   2021-04-08   Port Ericaburgh, AA   \n",
       "3   2021-04-08     East Seanview, AP   \n",
       "4   2021-04-08   North Jamieview, AP   \n",
       "..         ...                   ...   \n",
       "95  2021-04-08      Lake Abigail, AE   \n",
       "96  2021-04-08        Jacobshire, AP   \n",
       "97  2021-04-08        Port Susan, AE   \n",
       "98  2021-04-08     North Tiffany, AA   \n",
       "99  2021-04-08     Michelleville, AP   \n",
       "\n",
       "                                                  url  \\\n",
       "0   https://realpython.github.io/fake-jobs/jobs/se...   \n",
       "1   https://realpython.github.io/fake-jobs/jobs/en...   \n",
       "2   https://realpython.github.io/fake-jobs/jobs/le...   \n",
       "3   https://realpython.github.io/fake-jobs/jobs/fi...   \n",
       "4   https://realpython.github.io/fake-jobs/jobs/pr...   \n",
       "..                                                ...   \n",
       "95  https://realpython.github.io/fake-jobs/jobs/mu...   \n",
       "96  https://realpython.github.io/fake-jobs/jobs/ra...   \n",
       "97  https://realpython.github.io/fake-jobs/jobs/da...   \n",
       "98  https://realpython.github.io/fake-jobs/jobs/fu...   \n",
       "99  https://realpython.github.io/fake-jobs/jobs/sh...   \n",
       "\n",
       "                                          description  \n",
       "0   Professional asset web application environment...  \n",
       "1   Party prevent live. Quickly candidate change a...  \n",
       "2   Administration even relate head color. Staff b...  \n",
       "3   Tv program actually race tonight themselves tr...  \n",
       "4   Traditional page a although for study anyone. ...  \n",
       "..                                                ...  \n",
       "95  Paper age physical current note. There reality...  \n",
       "96  Able such right culture. Wrong pick structure ...  \n",
       "97  Create day party decade high clear. Past trade...  \n",
       "98  Pressure under rock next week. Recognize so re...  \n",
       "99  Management common popular project only. Must s...  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8a9e3-77e2-45fa-a40d-8c0e51d7e753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
